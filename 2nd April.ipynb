{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d2a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "# Answer: Grid search cross-validation is used to systematically evaluate the performance of a model for \n",
    "#     different hyperparameter combinations. It works by defining a grid of hyperparameters to search over,\n",
    "#     and then exhaustively tries all combinations of these hyperparameters to find the best model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cbc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Describe the difference between grid search cv and randomize search cv, and when might you choose \n",
    "#     one over the other?\n",
    "\n",
    "# Answer: Grid search cv exhaustively searches through all hyperparameter combinations specified in a grid, while \n",
    "#     randomized search cv samples a fixed number of hyperparameter combinations from specified probability distributions.\n",
    "#     Grid search is suitable when the search space is small and the computational resources are sufficient, while randomized\n",
    "#     search is preferred for larger search spaces or when computational resources are limited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62eb959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "# Answer: Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly \n",
    "#     optimistic performance estimates. An example is when features that contain information about the target variable are\n",
    "#     inadvertently included in the training data, such as using future information to predict past events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1c32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "# Answer: Data leakage can be prevented by ensuring that features used for modeling do not contain information about the target \n",
    "#     variable that would not be available at the time of prediction. This can be achieved by careful feature engineering, \n",
    "#     feature selection, and validation strategies such as using proper cross-validation techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96cb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5: What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "# Answer: A confusion matrix is a table that describes the performance of a classification model by comparing predicted classes \n",
    "#     with actual classes. It tells you the number of true positives, true negatives, false positives, and false negatives, \n",
    "#     allowing you to evaluate the model's accuracy, precision, recall, and other performance metrics.\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ce4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6: Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "# Answer: Precision is the ratio of true positives to the total number of predicted positives, indicating the proportion of \n",
    "#     correctly predicted positive instances among all predicted positive instances. Recall, on the other hand, is the ratio of\n",
    "#     true positives to the total number of actual positives, indicating the proportion of correctly predicted positive instances \n",
    "#     among all actual positive instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325f2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7: How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "# Answer: By examining the entries of the confusion matrix, you can determine which types of errors your model is making. For \n",
    "#     example, false positives indicate instances incorrectly classified as positive, while false negatives indicate instances \n",
    "#     incorrectly classified as negative.\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b9eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8: What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
    "\n",
    "# Answer: Common metrics derived from a confusion matrix include accuracy, precision, recall, F1-score, and specificity. They are\n",
    "#     calculated using formulas based on the values in the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b630c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9: What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "# Answer: Accuracy represents the overall correctness of the model and is calculated as the ratio of correct predictions to the\n",
    "#     total number of predictions. The values in the confusion matrix contribute to calculating accuracy, as accuracy is derived\n",
    "#     from the diagonal elements of the confusion matrix (true positives and true negatives).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35769ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10: How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n",
    "\n",
    "# Answer: By examining the entries of the confusion matrix, you can identify patterns of misclassification and understand which \n",
    "#     classes are more prone to errors. This helps in identifying potential biases or limitations in the model, such as class \n",
    "#     imbalances or issues with feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f622e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
